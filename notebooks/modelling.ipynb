{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library and data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spektral.data import Dataset, Graph, BatchLoader\n",
    "import dataset\n",
    "reload(dataset)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from spektral.layers import GATConv, GCNConv, GlobalAvgPool, GlobalMaxPool, GlobalSumPool, GlobalAttentionPool\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined data\n",
    "df_raw = pd.read_csv('../data/combined.csv')\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Load hero feature data\n",
    "df_features = pd.read_csv('../data/features.csv')\n",
    "df_features = df_features.set_index('hero_id')\n",
    "\n",
    "# Load standard filter\n",
    "df_filters = pd.read_csv('../models/filters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph dataset 50000 matches at a time\n",
    "dir = '../data/graphs_v1_scaled/'\n",
    "count = 0\n",
    "total = len(df)\n",
    "step = 50000\n",
    "\n",
    "for i in range(0,int(np.ceil(total/step))):\n",
    "    start = i*step\n",
    "    end = start+step-1 if (start+step)<total else total-1\n",
    "    path = dir+f'graphs_v1_scaled_{start}-{end}.pkl'\n",
    "    print(path)\n",
    "    file = open(path,'rb')\n",
    "    if i==0:\n",
    "        graphs = pickle.load(file)\n",
    "    else:\n",
    "        graphs = graphs + pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN IF DOING MODEL 1.7 FEATURE SELECTION (it will remove all desired features itself)\n",
    "# Remove attack_backswing feature\n",
    "for i in range(0,len(graphs)):\n",
    "    # if(i%100000==0):\n",
    "    graphs[i].x = graphs[i].x[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19]] # remove attack_backswing as a feature\n",
    "print('Attack backswing feature removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filt_idx(filt):\n",
    "    '''Returns indices of desired matches given a boolean array filter e.g. True, False, True returns [0,2]'''\n",
    "    # DotaV1 data handling (two graphs for every match: 0-49999 radiant, 0-49999 dire, 50000-99999 radiant, etc.)\n",
    "    step = 50000\n",
    "    filt_vals = []\n",
    "    for i in range(0,int(np.ceil(len(filt)/step))):\n",
    "        start = i*step\n",
    "        end = start+step\n",
    "        # Add filters for match range twice, as matches repeated every 50000\n",
    "        filt_vals = np.append(filt_vals, filt[start:end])\n",
    "        filt_vals = np.append(filt_vals, filt[start:end])\n",
    "\n",
    "    # Get indices of True values in filters\n",
    "    filt_idx = [i for i, x in enumerate(filt_vals) if x]\n",
    "    return filt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN IF DOING MODEL 1.9 MMR RANGES OR 1.10 DURATION RANGES (they handle filtering themselves)\n",
    "# Filter graph dataset\n",
    "filt = df_filters['filt_std'].values\n",
    "filt_idx = get_filt_idx(filt)\n",
    "graphs_filt = graphs[filt_idx]\n",
    "print('Standard filtering complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 64.0%\n",
      "Validation data: 16.0%\n",
      "Test data: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Train/valid/test split\n",
    "d = graphs_filt # Graph data\n",
    "\n",
    "np.random.seed(10)\n",
    "idxs = np.random.permutation(len(d))\n",
    "split_va, split_te = int(0.64 * len(d)), int(0.8 * len(d)) #64% training, 16% validation, 20% test\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "data_tr = d[idx_tr]\n",
    "data_va = d[idx_va]\n",
    "data_te = d[idx_te]\n",
    "\n",
    "print(f'Training data: {np.round(len(data_tr)/len(graphs_filt),2)*100}%')\n",
    "print(f'Validation data: {np.round(len(data_va)/len(graphs_filt),2)*100}%')\n",
    "print(f'Test data: {np.round(len(data_te)/len(graphs_filt),2)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm GPUs are being identified (requires tf environment, not the pipenv dotaprediction)\n",
    "# print(\"Num GPUs Available: \", list_physical_devices('GPU'))\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.0 - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_0(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(19, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x) \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_0 = Net_1_0()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_0.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_0 = model_1_0.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_0.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_0.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_0.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_0_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_0.save(f'../models/model_1_0.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.1 - GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_1(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(19, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x =self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_1 = Net_1_1()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_1.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_1 = model_1_1.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_1.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_1.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_1.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_1_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_1.save(f'../models/model_1_1.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.2 Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_2(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(19, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_2 = Net_1_2()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_2.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_2 = model_1_2.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_2.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_2.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_2.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_2_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_2.save(f'../models/model_1_2.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.3 - Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_3(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(19, activation='relu')\n",
    "        self.pool1 = GlobalMaxPool()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_3 = Net_1_3()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_3.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_3 = model_1_3.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_3.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_3.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_3.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_3_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_3.save(f'../models/model_1_3.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.4 - Sum Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_4(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(19, activation='relu')\n",
    "        self.pool1 = GlobalSumPool()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)        \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_4 = Net_1_4()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_4.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_4 = model_1_4.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_4.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_4.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_4.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_4_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_4.save(f'../models/model_1_4.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.5 - Attention Pooling - not presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_5(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(19, activation='relu')\n",
    "        self.pool1 = GlobalAttentionPool(19)\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_5 = Net_1_5()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_5.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_5 = model_1_5.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_5.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_5.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_5.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_5_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_5.save(f'../models/model_1_5.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.6 GATConv + Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_6(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(19, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_6 = Net_1_6()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_6.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_6 = model_1_6.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_6.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_6.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_6.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_6_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_6.save(f'../models/model_1_6.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.7 - Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each graph and scale feature matrix and drop following features: (col numbers are after removing attack_backswing (14) earlier)\n",
    "\n",
    "# base_attack_time (12)\n",
    "# attack_point (13)\n",
    "# vision_day (14)\n",
    "# vision_night (15)\n",
    "# turn_rate (16)\n",
    "# collision_size (17)\n",
    "\n",
    "print('Selecting specific columns:')\n",
    "for i in range(0,len(graphs_filt)): # match 0 has only 15 features, reason not known, skipping this\n",
    "    if(i%100000==0):\n",
    "        print(i)\n",
    "\n",
    "    graphs_filt[i].x = graphs_filt[i].x[:,[0,1,2,3,4,5,6,7,8,9,10,11,18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split\n",
    "d_fs = graphs_filt # Graph data\n",
    "\n",
    "# np.random.seed(10)\n",
    "idxs = np.random.permutation(len(d_fs))\n",
    "split_va, split_te = int(0.64 * len(d_fs)), int(0.8 * len(d_fs)) #64% training, 16% validation, 20% test\n",
    "# idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te]) # use indices from earlier split\n",
    "data_tr_fs = d_fs[idx_tr]\n",
    "data_va_fs = d_fs[idx_va]\n",
    "data_te_fs = d_fs[idx_te]\n",
    "\n",
    "print(f'Training data: {np.round(len(data_tr_fs)/len(graphs_filt),2)*100}%')\n",
    "print(f'Validation data: {np.round(len(data_va_fs)/len(graphs_filt),2)*100}%')\n",
    "print(f'Test data: {np.round(len(data_te_fs)/len(graphs_filt),2)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 50  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr_fs, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va_fs, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te_fs, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_7(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(13, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_7 = Net_1_7()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_7.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_7 = model_1_7.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_7.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_7.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_7.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv('../models/fit_records/model_1_7_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_7.save(f'../models/model_1_7.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.8 Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "# Loop through range of num channels hyperparameter (manual loop, auto was giving errors)\n",
    "channel = 15 # change manually through [5,10,15,25,30]\n",
    "channel_lookup = {5:1, 10:2, 15:3, 25:4, 30:5, 35:6}\n",
    "i = channel_lookup[channel]\n",
    "\n",
    "print(f'Channels: {channel}')\n",
    "\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_8(Model):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(channels, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.dense = Dense(d.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_8 = Net_1_8(channel)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_8.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_8 = model_1_8.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_8.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_8.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_8.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv(f'../models/fit_records/model_1_8_{i+1}_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_8.save(f'../models/model_1_8_{i+1}.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.9 MMR ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 30  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "group = 1 # choose 1-6\n",
    "\n",
    "# Filter data for current mmr group\n",
    "filt = df_filters['filt_std'].values & df_filters[f'filt_mmr_{group}'].values\n",
    "filt_idx = get_filt_idx(filt)\n",
    "graphs_filt_mmr = graphs[filt_idx]\n",
    "print('Filtered (standard + MMR group)')\n",
    "\n",
    "# Remove attack_backswing feature\n",
    "for i in range(0,len(graphs_filt_mmr)):\n",
    "    # if(i%100000==0):\n",
    "    graphs_filt_mmr[i].x = graphs_filt_mmr[i].x[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19]] # remove attack_backswing as a feature\n",
    "print('Attack backswing feature removed')\n",
    "\n",
    "# Train/valid split\n",
    "np.random.seed(10)\n",
    "idxs = np.random.permutation(len(graphs_filt_mmr))\n",
    "split_va, split_te = int(0.7 * len(graphs_filt_mmr)), int(len(graphs_filt_mmr)) #70% training, 30% validation\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "data_tr = graphs_filt_mmr[idx_tr]\n",
    "data_va = graphs_filt_mmr[idx_va]\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_9(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(15, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.dense = Dense(graphs_filt_mmr.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_9 = Net_1_9()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_9.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_9 = model_1_9.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_9.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_9.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_9.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv(f'../models/fit_records/model_1_9_{group}_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_9.save(f'../models/model_1_9_{group}.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches MMR group 1: 117735\n",
      "Matches MMR group 2: 450537\n",
      "Matches MMR group 3: 1158591\n",
      "Matches MMR group 4: 1332106\n",
      "Matches MMR group 5: 385337\n",
      "Matches MMR group 6: 44372\n"
     ]
    }
   ],
   "source": [
    "# Print number of matches in each MMR group\n",
    "for i in range(1,7):\n",
    "    filt = df_filters['filt_std'].values & df_filters[f'filt_mmr_{i}'].values\n",
    "    matches_mmr_group = len(filt[filt==True])\n",
    "    print(f'Matches MMR group {i}: {matches_mmr_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.10 Duration ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 30  # Number of training epochs\n",
    "batch_size = 256  # Batch size\n",
    "\n",
    "group = 1 # choose 1-6\n",
    "\n",
    "# Filter data for current duration group\n",
    "filt = df_filters['filt_std'].values & df_filters[f'filt_duration_{group}'].values\n",
    "filt_idx = get_filt_idx(filt)\n",
    "graphs_filt_duration = graphs[filt_idx]\n",
    "print('Filtered (standard + duration group)')\n",
    "\n",
    "# Train/valid split\n",
    "np.random.seed(10)\n",
    "idxs = np.random.permutation(len(graphs_filt_duration))\n",
    "split_va, split_te = int(0.7 * len(graphs_filt_duration)), int(len(graphs_filt_duration)) #70% training, 30% validation\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "data_tr = graphs_filt_duration[idx_tr]\n",
    "data_va = graphs_filt_duration[idx_va]\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_1_10(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(15, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.dense = Dense(graphs_filt_duration.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)       \n",
    "        return x\n",
    "\n",
    "# Train model\n",
    "model_1_10 = Net_1_10()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_1_10.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_1_10 = model_1_10.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training record\n",
    "epochs = list(range(1,len(fit_log_1_10.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_1_10.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_1_10.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv(f'../models/fit_records/model_1_10_{group}_accuracy.csv', index=False)\n",
    "\n",
    "# Pickle model and training+validation log\n",
    "filehandler = open(f'../models/fit_records/fit_log_1_10_{group}.pkl','wb')\n",
    "pickle.dump(fit_log_1_10, filehandler)\n",
    "filehandler = open(f'../models/fit_records/model_1_10_{group}.pkl','wb')\n",
    "pickle.dump(model_1_10, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "model_1_10.save(f'../models/model_1_10_{group}.tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches MMR group 1: 654790\n",
      "Matches MMR group 2: 1254998\n",
      "Matches MMR group 3: 1231033\n",
      "Matches MMR group 4: 1036095\n",
      "Matches MMR group 5: 573267\n",
      "Matches MMR group 6: 327147\n"
     ]
    }
   ],
   "source": [
    "# Print number of matches in each duration group\n",
    "for i in range(1,7):\n",
    "    filt = df_filters['filt_std'].values & df_filters[f'filt_duration_{i}'].values\n",
    "    matches_mmr_group = len(filt[filt==True])\n",
    "    print(f'Matches MMR group {i}: {matches_mmr_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model (1.8.3) - Test data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7935/7935 [==============================] - 56s 7ms/step - loss: 0.6896 - binary_accuracy: 0.5346\n",
      "Test accuracy: 53.459999999999994%\n"
     ]
    }
   ],
   "source": [
    "final_model = keras.models.load_model(f'../models/model_1_8_3.tf')\n",
    "loss = final_model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(f\"Test accuracy: {np.round(loss[1],4)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aead6205437b837a6f51f1a5578747fd76aee053b7788bc0930d4c5b4657d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

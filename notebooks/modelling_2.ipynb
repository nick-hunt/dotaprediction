{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataset\n",
    "reload(dataset)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from spektral.data import Dataset, Graph, BatchLoader\n",
    "from spektral.layers import ECCConv, EdgeConv, GlobalAvgPool\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined data\n",
    "df_raw = pd.read_csv('../data/combined.csv')\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Load hero feature data\n",
    "df_features = pd.read_csv('../data/features.csv')\n",
    "df_features = df_features.set_index('hero_id')\n",
    "\n",
    "# Load standard filter\n",
    "df_filters = pd.read_csv('../models/filters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph dataset 50000 matches at a time\n",
    "dir = '../data/graphs_v2_scaled/'\n",
    "count = 0\n",
    "total = len(df)\n",
    "step = 50000\n",
    "\n",
    "for i in range(0,int(np.ceil(total/step))):\n",
    "    start = i*step\n",
    "    end = start+step-1 if (start+step)<total else total-1\n",
    "    path = dir+f'graphs_v2_scaled_{start}-{end}.pkl'\n",
    "    print(path)\n",
    "    file = open(path,'rb')\n",
    "    if i==0:\n",
    "        graphs = pickle.load(file)\n",
    "    else:\n",
    "        graphs = graphs + pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove attack_backswing feature\n",
    "for i in range(0,len(graphs)):\n",
    "    # if(i%100000==0):\n",
    "    graphs[i].x = graphs[i].x[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19]] # remove attack_backswing as a feature\n",
    "print('Attack backswing feature removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filt_idx(filt):\n",
    "    '''Returns indices of desired matches given a boolean array filter e.g. True, False, True returns [0,2]'''\n",
    "    # DotaV2 data handling (one graphs for every match: 0-49999, 50000-99999, etc.)\n",
    "    # Get indices of True values in filters\n",
    "    filt_idx = [i for i, x in enumerate(filt) if x]\n",
    "    return filt_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.0 - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL:\n",
    "# Redefine edge features from single value 1 or 2 to hot encoded depth=2, so 1 = [1,0] and 2 = [0,1]\n",
    "# for i, graph in enumerate(graphs):\n",
    "#     if i%100000==0:\n",
    "#         print(i)\n",
    "#     e_cur = graph.e\n",
    "#     e_new = np.zeros(shape=(10,10,2)) # empty edge feature matrix, single feature (team mates=1, enemy=2)\n",
    "#     for row in range(0,10):\n",
    "#         for col in range(0,10):\n",
    "#             if e_cur[row,col,0]==1:\n",
    "#                 e_new[row,col,0], e_new[row,col,1] = 1,0\n",
    "#             else:\n",
    "#                 e_new[row,col,0], e_new[row,col,1] = 0,1\n",
    "#     graph.e = e_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL:\n",
    "# Redefine edge features from single value 1 or 2 to 1 or 0 (single binary feature: teammate)\n",
    "# for i, graph in enumerate(graphs):\n",
    "#     if i%100000==0:\n",
    "#         print(i)\n",
    "#     e_cur = graph.e\n",
    "#     e_new = np.ones(shape=(10,10,1)) # empty edge feature matrix, single feature (team mates=1, enemy=2)\n",
    "#     for row in range(0,10):\n",
    "#         for col in range(0,10):\n",
    "#             if e_cur[row,col,0]==2:\n",
    "#                 e_new[row,col,0] = 0\n",
    "#     graph.e = e_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "learning_rate = 0.001  # Learning rate\n",
    "epochs = 30  # Number of training epochs\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 256 # Batch size\n",
    "\n",
    "# Filtering graphs (standard only)\n",
    "filt = df_filters['filt_std'].values\n",
    "filt_idx = get_filt_idx(filt)\n",
    "graphs_filt = graphs[filt_idx]\n",
    "print('Filtered (standard only)')\n",
    "\n",
    "# Train/valid/test split\n",
    "idxs = np.random.permutation(len(graphs_filt))\n",
    "split_va, split_te = int(0.64 * len(graphs_filt)), int(0.8 * len(graphs_filt))\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "data_tr = graphs_filt[idx_tr]\n",
    "data_va = graphs_filt[idx_va]\n",
    "data_te = graphs_filt[idx_te]\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = BatchLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = BatchLoader(data_va, batch_size=batch_size)\n",
    "loader_te = BatchLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class Net_2_0(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ECCConv(19, activation='relu')\n",
    "        self.pool1 = GlobalAvgPool()\n",
    "        self.dense = Dense(graphs_filt.n_labels, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, e = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        x = self.pool1(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Compile and train model\n",
    "model_2_0 = Net_2_0()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = BinaryCrossentropy()\n",
    "model_2_0.compile(optimizer=optimizer, loss=loss_fn, metrics=['binary_accuracy'])\n",
    "fit_log_2_0 = model_2_0.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch)\n",
    "\n",
    "# Save training and validation results\n",
    "epochs = list(range(1,len(fit_log_2_0.history['binary_accuracy'])+1))\n",
    "training_accuracy = fit_log_2_0.history['binary_accuracy']\n",
    "validation_accuracy = fit_log_2_0.history['val_binary_accuracy']\n",
    "pd.DataFrame({'epoch': epochs, 'training_accuracy':training_accuracy, 'validation_accuracy':validation_accuracy}).to_csv(f'../models/fit_records/model_2_0_accuracy.csv', index=False)\n",
    "\n",
    "# Pickle model and training+validation log\n",
    "filehandler = open(f'../models/fit_records/fit_log_2_0.pkl','wb')\n",
    "pickle.dump(fit_log_2_0, filehandler)\n",
    "filehandler = open(f'../models/fit_records/model_2_0.pkl','wb')\n",
    "pickle.dump(model_2_0, filehandler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aead6205437b837a6f51f1a5578747fd76aee053b7788bc0930d4c5b4657d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
